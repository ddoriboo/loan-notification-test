#!/usr/bin/env python3
"""
ì§„ì§œ LLM ê¸°ë°˜ ë¬¸êµ¬ ìƒì„±ê¸°
- OpenAI GPT API ì—°ë™
- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- ê¸°ì¡´ ë°ì´í„° í•™ìŠµ í™œìš©
- íƒ€ì´ë° ìµœì í™” í†µí•©
"""

import json
import csv
from datetime import datetime, timedelta
import statistics
from enhanced_timing_analyzer import EnhancedTimingAnalyzer
import re
import os

# OpenAI API (ì‹¤ì œ ì‚¬ìš© ì‹œ ì„¤ì¹˜ í•„ìš”)
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âŒ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

class RealLLMGenerator:
    def __init__(self, csv_file, openai_api_key=None):
        self.csv_file = csv_file
        self.data = []
        self.timing_analyzer = EnhancedTimingAnalyzer(csv_file)
        self.high_performance_messages = []
        self.performance_patterns = {}
        
        # OpenAI API í‚¤ í™•ì¸ (í™˜ê²½ë³€ìˆ˜ ìš°ì„ )
        api_key = openai_api_key or os.environ.get('OPENAI_API_KEY')
        
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Railway í™˜ê²½ë³€ìˆ˜ì— OPENAI_API_KEYë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        
        # OpenAI ì„¤ì •
        openai.api_key = api_key
        self.llm_available = True
        print("âœ… OpenAI API ì—°ê²° ì™„ë£Œ")
        
        self.load_and_analyze_data()
        
    def load_and_analyze_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ë¶„ì„"""
        print("ğŸ“Š ë°ì´í„° ë¶„ì„ ì¤‘...")
        
        with open(self.csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    row['í´ë¦­ìœ¨'] = float(row['í´ë¦­ìœ¨'])
                    row['ë°œì†¡íšŒì›ìˆ˜'] = int(row['ë°œì†¡íšŒì›ìˆ˜'])
                    row['í´ë¦­íšŒì›ìˆ˜'] = int(row['í´ë¦­íšŒì›ìˆ˜'])
                    row['ë°œì†¡ì¼'] = datetime.strptime(row['ë°œì†¡ì¼'], '%Y-%m-%d')
                    self.data.append(row)
                    
                    # ê³ ì„±ê³¼ ë©”ì‹œì§€ ìˆ˜ì§‘ (í´ë¦­ë¥  ìƒìœ„ 20%)
                    if row['í´ë¦­ìœ¨'] > 12:  # ì„ê³„ê°’
                        self.high_performance_messages.append(row)
                except (ValueError, KeyError):
                    continue
        
        # ì„±ê³¼ íŒ¨í„´ ë¶„ì„
        self.analyze_performance_patterns()
        print(f"âœ… ì´ {len(self.data)}ê°œ ë©”ì‹œì§€ ë¶„ì„ ì™„ë£Œ")
        print(f"ğŸ† ê³ ì„±ê³¼ ë©”ì‹œì§€ {len(self.high_performance_messages)}ê°œ ì‹ë³„")
    
    def analyze_performance_patterns(self):
        """ì„±ê³¼ íŒ¨í„´ ë¶„ì„"""
        # ì„œë¹„ìŠ¤ë³„ ìµœê³  ì„±ê³¼ ë©”ì‹œì§€
        service_best = {}
        for row in self.high_performance_messages:
            service = row['ì„œë¹„ìŠ¤ëª…']
            if service not in service_best or row['í´ë¦­ìœ¨'] > service_best[service]['í´ë¦­ìœ¨']:
                service_best[service] = row
        
        # í‚¤ì›Œë“œë³„ ì„±ê³¼
        keyword_performance = {}
        keywords = ['í˜œíƒ', 'ìµœëŒ€', 'í• ì¸', 'ê¸ˆë¦¬', 'í•œë„', 'ëŒ€ì¶œ', 'ë¹„êµ', 'ê°ˆì•„íƒ€ê¸°', 'í™•ì¸', 'ì‹ ì²­']
        
        for keyword in keywords:
            keyword_messages = [row for row in self.data if keyword in row['ë°œì†¡ ë¬¸êµ¬']]
            if keyword_messages:
                avg_rate = statistics.mean([row['í´ë¦­ìœ¨'] for row in keyword_messages])
                keyword_performance[keyword] = {
                    'avg_rate': avg_rate,
                    'count': len(keyword_messages),
                    'best_message': max(keyword_messages, key=lambda x: x['í´ë¦­ìœ¨'])
                }
        
        self.performance_patterns = {
            'service_best': service_best,
            'keyword_performance': keyword_performance,
            'overall_avg': statistics.mean([row['í´ë¦­ìœ¨'] for row in self.data])
        }
    
    def create_llm_prompt(self, user_request):
        """LLM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # íƒ€ì´ë° ë¶„ì„ ê²°ê³¼
        timing_rec = self.timing_analyzer.get_optimal_timing_recommendation()
        
        # ê´€ë ¨ ê³ ì„±ê³¼ ë©”ì‹œì§€ ì„ ë³„
        relevant_messages = self.get_relevant_high_performance_messages(user_request)
        
        # ì„±ê³¼ íŒ¨í„´ ìš”ì•½
        top_keywords = sorted(
            self.performance_patterns['keyword_performance'].items(),
            key=lambda x: x[1]['avg_rate'],
            reverse=True
        )[:5]
        
        prompt = f"""
ë‹¹ì‹ ì€ ëŒ€ì¶œ ì„œë¹„ìŠ¤ ì „ë¬¸ ë§ˆì¼€íŒ… ë¬¸êµ¬ ì‘ì„±ìì…ë‹ˆë‹¤. 18ê°œì›”ê°„ì˜ ì‹¤ì œ ì•Œë¦¼ ë°œì†¡ ë°ì´í„°ë¥¼ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ë¬¸êµ¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

## ğŸ“Š ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
- ì´ ë¶„ì„ ë°ì´í„°: {len(self.data)}ê°œ ë©”ì‹œì§€
- ì „ì²´ í‰ê·  í´ë¦­ë¥ : {self.performance_patterns['overall_avg']:.2f}%
- ìµœê³  ì„±ê³¼ í‚¤ì›Œë“œ: {', '.join([f"'{k}'({v['avg_rate']:.1f}%)" for k, v in top_keywords])}

## ğŸ¯ ê³ ì„±ê³¼ ë©”ì‹œì§€ ì‚¬ë¡€
"""
        
        for i, msg_data in enumerate(relevant_messages[:3], 1):
            prompt += f"{i}. \"{msg_data['ë°œì†¡ ë¬¸êµ¬']}\" (í´ë¦­ë¥ : {msg_data['í´ë¦­ìœ¨']:.1f}%)\n"
        
        prompt += f"""
## â° ìµœì  íƒ€ì´ë° ë°ì´í„°
- ìµœê³  ì„±ê³¼ ìš”ì¼: {timing_rec['best_weekday']}
- ìµœê³  ì„±ê³¼ ì›”êµ¬ê°„: {timing_rec['best_monthly_period']}
- ê¸‰ì—¬ì¼ ì—°ê´€ì„±: {timing_rec['best_payday_timing']}

## ğŸ“ ì‚¬ìš©ì ìš”ì²­
{user_request.get('description', '')}

íƒ€ê²Ÿ ê³ ê°: {user_request.get('target_audience', 'ì¼ë°˜')}
ì„œë¹„ìŠ¤ ìœ í˜•: {user_request.get('service', 'ì „ì²´')}
ì›í•˜ëŠ” í†¤: {user_request.get('tone', 'í˜œíƒ ê°•ì¡°í˜•')}
í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(user_request.get('keywords', []))}

## ğŸ¯ ìƒì„± ìš”êµ¬ì‚¬í•­
1. ìœ„ì˜ ê³ ì„±ê³¼ ë©”ì‹œì§€ íŒ¨í„´ì„ ì°¸ê³ í•˜ë˜, ì™„ì „íˆ ìƒˆë¡œìš´ ë¬¸êµ¬ ìƒì„±
2. ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ì„ ì •í™•íˆ ë°˜ì˜
3. ìµœì  íƒ€ì´ë°ì— ë§ëŠ” ê¸´ê¸‰ì„±/í˜œíƒ ì¡°ì ˆ
4. ê²€ì¦ëœ ê³ ì„±ê³¼ í‚¤ì›Œë“œ ì ê·¹ í™œìš©
5. (ê´‘ê³ ) í‘œì‹œ í¬í•¨ í•„ìˆ˜

## ğŸ“‹ ì¶œë ¥ í˜•ì‹
ë‹¤ìŒê³¼ ê°™ì´ 3ê°œì˜ ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”:

**ìŠ¤íƒ€ì¼ 1: í˜œíƒ ê°•ì¡°í˜•**
ë¬¸êµ¬: [ìƒì„±ëœ ë¬¸êµ¬]
ì˜ˆìƒ í´ë¦­ë¥ : [%]
ìƒì„± ê·¼ê±°: [ì™œ ì´ë ‡ê²Œ ìƒì„±í–ˆëŠ”ì§€ ì„¤ëª…]

**ìŠ¤íƒ€ì¼ 2: ê¸´ê¸‰ì„± ê°•ì¡°í˜•**  
ë¬¸êµ¬: [ìƒì„±ëœ ë¬¸êµ¬]
ì˜ˆìƒ í´ë¦­ë¥ : [%]
ìƒì„± ê·¼ê±°: [ì™œ ì´ë ‡ê²Œ ìƒì„±í–ˆëŠ”ì§€ ì„¤ëª…]

**ìŠ¤íƒ€ì¼ 3: ê°œì¸í™” ë§ì¶¤í˜•**
ë¬¸êµ¬: [ìƒì„±ëœ ë¬¸êµ¬] 
ì˜ˆìƒ í´ë¦­ë¥ : [%]
ìƒì„± ê·¼ê±°: [ì™œ ì´ë ‡ê²Œ ìƒì„±í–ˆëŠ”ì§€ ì„¤ëª…]

ê° ë¬¸êµ¬ëŠ” ê¸°ì¡´ ê³ ì„±ê³¼ ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ë˜ ì™„ì „íˆ ìƒˆë¡œìš´ ì°½ì˜ì  í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.
"""
        
        return prompt
    
    def get_relevant_high_performance_messages(self, user_request):
        """ì‚¬ìš©ì ìš”ì²­ê³¼ ê´€ë ¨ëœ ê³ ì„±ê³¼ ë©”ì‹œì§€ ì„ ë³„"""
        keywords = user_request.get('keywords', [])
        service = user_request.get('service', '')
        
        scored_messages = []
        
        for msg_data in self.high_performance_messages:
            score = 0
            message = msg_data['ë°œì†¡ ë¬¸êµ¬']
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            for keyword in keywords:
                if keyword in message:
                    score += 2
            
            # ì„œë¹„ìŠ¤ ë§¤ì¹­ ì ìˆ˜
            if service and service in msg_data['ì„œë¹„ìŠ¤ëª…']:
                score += 3
            
            # í´ë¦­ë¥  ë³´ë„ˆìŠ¤
            score += msg_data['í´ë¦­ìœ¨'] / 10
            
            if score > 0:
                scored_messages.append({
                    **msg_data,
                    'relevance_score': score
                })
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        scored_messages.sort(key=lambda x: x['relevance_score'], reverse=True)
        return scored_messages[:5]
    
    def call_llm_api(self, prompt):
        """ì‹¤ì œ LLM API í˜¸ì¶œ"""
        try:
            # OpenAI API v1.0+ í˜•ì‹
            client = openai.OpenAI(api_key=openai.api_key)
            response = client.chat.completions.create(
                model="gpt-4o-2024-11-20",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ê¸°ë°˜ì˜ ì „ë¬¸ ë§ˆì¼€íŒ… ë¬¸êµ¬ ì‘ì„±ìì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except AttributeError:
            # êµ¬ë²„ì „ OpenAI API í˜•ì‹
            try:
                response = openai.ChatCompletion.create(
                    model="gpt-4o-2024-11-20",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ê¸°ë°˜ì˜ ì „ë¬¸ ë§ˆì¼€íŒ… ë¬¸êµ¬ ì‘ì„±ìì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=2000,
                    temperature=0.7
                )
                return response.choices[0].message.content
            except Exception as e:
                raise Exception(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
                
        except Exception as e:
            raise Exception(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    def simulate_llm_response(self, prompt):
        """LLM ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜ (API ì—†ì„ ë•Œ)"""
        print("ğŸ¤– LLM ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜ ì¤‘...")
        
        # í”„ë¡¬í”„íŠ¸ì—ì„œ ì‚¬ìš©ì ìš”ì²­ íŒŒì‹±
        user_keywords = re.findall(r'í•µì‹¬ í‚¤ì›Œë“œ: ([^\n]+)', prompt)
        target_audience = re.findall(r'íƒ€ê²Ÿ ê³ ê°: ([^\n]+)', prompt)
        service_type = re.findall(r'ì„œë¹„ìŠ¤ ìœ í˜•: ([^\n]+)', prompt)
        
        keywords = user_keywords[0].split(', ') if user_keywords else ['í˜œíƒ']
        target = target_audience[0] if target_audience else 'ê³ ê°'
        service = service_type[0] if service_type else 'ëŒ€ì¶œ'
        
        # ë°ì´í„° ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        responses = [
            {
                'style': 'í˜œíƒ ê°•ì¡°í˜•',
                'message': f"(ê´‘ê³ ) {target}ë‹˜ë§Œì„ ìœ„í•œ íŠ¹ë³„ {', '.join(keywords[:2])} í˜œíƒ! {service} í™•ì¸í•˜ê³  ìµœëŒ€ í˜œíƒ ë°›ê¸° ğŸ‘‰",
                'predicted_rate': 11.8,
                'reasoning': f"ê³ ì„±ê³¼ í‚¤ì›Œë“œ '{keywords[0]}' í™œìš©, {target} ë§ì¶¤ í‘œí˜„ìœ¼ë¡œ ê°œì¸í™”, ëª…í™•í•œ CTAë¡œ í´ë¦­ ìœ ë„"
            },
            {
                'style': 'ê¸´ê¸‰ì„± ê°•ì¡°í˜•', 
                'message': f"(ê´‘ê³ ) âš¡ ë§ˆê°ì„ë°•! {service} {keywords[0]} ê¸°íšŒë¥¼ ë†“ì¹˜ì§€ ë§ˆì„¸ìš”. ì§€ê¸ˆ ë°”ë¡œ í™•ì¸í•˜ê¸°",
                'predicted_rate': 10.5,
                'reasoning': "ê¸´ê¸‰ì„± í‚¤ì›Œë“œë¡œ ì¦‰ì‹œ í–‰ë™ ìœ ë„, ì†ì‹¤ íšŒí”¼ ì‹¬ë¦¬ í™œìš©, ê°„ê²°í•œ êµ¬ì¡°ë¡œ ì§‘ì¤‘ë„ í–¥ìƒ"
            },
            {
                'style': 'ê°œì¸í™” ë§ì¶¤í˜•',
                'message': f"(ê´‘ê³ ) {target}ë‹˜ì˜ ì¡°ê±´ì— ë”± ë§ëŠ” {service} ì°¾ì•˜ì–´ìš”! {', '.join(keywords)} í™•ì¸í•˜ê³  ë§ì¶¤ í˜œíƒ ë°›ê¸°",
                'predicted_rate': 12.3,
                'reasoning': f"ê°œì¸í™”ëœ ë©”ì‹œì§€ë¡œ ê´€ë ¨ì„± í–¥ìƒ, '{target}' ì§ì ‘ ì–¸ê¸‰ìœ¼ë¡œ ì£¼ì˜ ì§‘ì¤‘, ë‹¤ì¤‘ í‚¤ì›Œë“œ í™œìš©"
            }
        ]
        
        # ì‘ë‹µ í¬ë§·íŒ…
        formatted_response = ""
        for resp in responses:
            formatted_response += f"""
**ìŠ¤íƒ€ì¼ {resp['style']}**
ë¬¸êµ¬: {resp['message']}
ì˜ˆìƒ í´ë¦­ë¥ : {resp['predicted_rate']}%
ìƒì„± ê·¼ê±°: {resp['reasoning']}

"""
        
        return formatted_response
    
    def parse_llm_response(self, llm_response):
        """LLM ì‘ë‹µ íŒŒì‹±"""
        generated_messages = []
        
        # ìŠ¤íƒ€ì¼ë³„ ì‘ë‹µ íŒŒì‹±
        styles = re.findall(r'\*\*ìŠ¤íƒ€ì¼ (.*?)\*\*\në¬¸êµ¬: (.*?)\nì˜ˆìƒ í´ë¦­ë¥ : (.*?)%\nìƒì„± ê·¼ê±°: (.*?)\n', 
                          llm_response, re.DOTALL)
        
        for style, message, rate, reasoning in styles:
            try:
                generated_messages.append({
                    'style': style.strip(),
                    'message': message.strip(),
                    'predicted_rate': float(rate.strip()),
                    'reasoning': reasoning.strip(),
                    'source': 'real_llm' if self.llm_available else 'simulated_llm'
                })
            except ValueError:
                continue
        
        return generated_messages
    
    def get_optimal_timing_for_message(self, user_request):
        """ë©”ì‹œì§€ë³„ ìµœì  íƒ€ì´ë° ê³„ì‚°"""
        service = user_request.get('service', 'ì „ì²´')
        
        # ì„œë¹„ìŠ¤ë³„ ìµœì  íƒ€ì´ë° ê°€ì ¸ì˜¤ê¸°
        timing_rec = self.timing_analyzer.get_optimal_timing_recommendation(
            target_service=service
        )
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ ë‹¤ìŒ ìµœì  ë°œì†¡ì¼ ê³„ì‚°
        today = datetime.now()
        
        # ìˆ˜ìš”ì¼ ì°¾ê¸°
        days_ahead = 2 - today.weekday()  # ìˆ˜ìš”ì¼ = 2
        if days_ahead <= 0:
            days_ahead += 7
        
        next_optimal = today + timedelta(days=days_ahead)
        
        # ì›”ì´ˆ ì¡°ì • (1-10ì¼)
        if next_optimal.day > 10:
            # ë‹¤ìŒ ë‹¬ ì›”ì´ˆë¡œ ì¡°ì •
            if next_optimal.month == 12:
                next_optimal = datetime(next_optimal.year + 1, 1, 3)  # ë‹¤ìŒí•´ 1ì›” 3ì¼(ìˆ˜ìš”ì¼ ê°€ì •)
            else:
                next_optimal = datetime(next_optimal.year, next_optimal.month + 1, 3)
        
        return {
            'optimal_date': next_optimal.strftime('%Y-%m-%d'),
            'optimal_time': '10:00',  # ì˜¤ì „ 10ì‹œ
            'weekday': timing_rec['best_weekday'],
            'monthly_period': timing_rec['best_monthly_period'],
            'reasoning': f"{timing_rec['best_weekday']} + {timing_rec['best_monthly_period']} ì¡°í•©ì´ ìµœì "
        }
    
    def generate_with_llm(self, user_request):
        """LLM ê¸°ë°˜ ë¬¸êµ¬ ìƒì„±"""
        print("ğŸ¤– LLM ê¸°ë°˜ ë¬¸êµ¬ ìƒì„± ì¤‘...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.create_llm_prompt(user_request)
        
        # LLM í˜¸ì¶œ
        llm_response = self.call_llm_api(prompt)
        
        # ì‘ë‹µ íŒŒì‹±
        generated_messages = self.parse_llm_response(llm_response)
        
        # ìµœì  íƒ€ì´ë° ê³„ì‚°
        optimal_timing = self.get_optimal_timing_for_message(user_request)
        
        # ê¸°ì¡´ ë©”ì‹œì§€ ë§¤ì¹­ (ë¹„êµìš©)
        relevant_existing = self.get_relevant_high_performance_messages(user_request)
        
        result = {
            'user_request': user_request,
            'generated_messages': generated_messages,
            'optimal_timing': optimal_timing,
            'relevant_existing_messages': relevant_existing[:3],
            'llm_raw_response': llm_response,
            'data_insights': {
                'total_analyzed': len(self.data),
                'high_performance_count': len(self.high_performance_messages),
                'average_click_rate': self.performance_patterns['overall_avg'],
                'top_keywords': list(self.performance_patterns['keyword_performance'].keys())[:5]
            }
        }
        
        return result
    
    def compare_with_existing(self, user_request):
        """ê¸°ì¡´ ë©”ì‹œì§€ì™€ LLM ìƒì„± ë©”ì‹œì§€ ë¹„êµ"""
        print("âš–ï¸ ê¸°ì¡´ ë©”ì‹œì§€ vs LLM ìƒì„± ë©”ì‹œì§€ ë¹„êµ...")
        
        # LLM ìƒì„±
        llm_result = self.generate_with_llm(user_request)
        
        # ê¸°ì¡´ ë©”ì‹œì§€
        existing_messages = self.get_relevant_high_performance_messages(user_request)
        
        comparison = {
            'llm_generated': llm_result['generated_messages'],
            'existing_high_performance': existing_messages[:3],
            'timing_optimization': llm_result['optimal_timing'],
            'winner_prediction': self.predict_winner(
                llm_result['generated_messages'], 
                existing_messages[:3]
            )
        }
        
        return comparison
    
    def predict_winner(self, llm_messages, existing_messages):
        """ìŠ¹ì ì˜ˆì¸¡"""
        llm_avg = statistics.mean([msg['predicted_rate'] for msg in llm_messages]) if llm_messages else 0
        existing_avg = statistics.mean([msg['í´ë¦­ìœ¨'] for msg in existing_messages]) if existing_messages else 0
        
        if llm_avg > existing_avg:
            return {
                'winner': 'LLM ìƒì„±',
                'advantage': f"{llm_avg - existing_avg:.1f}%p",
                'reason': 'LLMì´ ê°œì¸í™”ì™€ ìµœì‹  íŒ¨í„´ì„ ë” ì˜ ë°˜ì˜'
            }
        else:
            return {
                'winner': 'ê¸°ì¡´ ê²€ì¦',
                'advantage': f"{existing_avg - llm_avg:.1f}%p", 
                'reason': 'ì‹¤ì œ ê²€ì¦ëœ ë°ì´í„°ì˜ ì‹ ë¢°ì„±ì´ ë†’ìŒ'
            }
    
    def get_dashboard_data(self):
        """ëŒ€ì‹œë³´ë“œìš© ë¶„ì„ ë°ì´í„° ë°˜í™˜"""
        # ì„œë¹„ìŠ¤ë³„ ë¶„ì„
        service_analysis = {}
        for msg_data in self.data:
            service = msg_data.get('ì„œë¹„ìŠ¤ëª…', 'ê¸°íƒ€')
            if service not in service_analysis:
                service_analysis[service] = {
                    'count': 0,
                    'total_clicks': 0,
                    'messages': []
                }
            service_analysis[service]['count'] += 1
            service_analysis[service]['total_clicks'] += float(msg_data.get('í´ë¦­ìœ¨', 0))
            service_analysis[service]['messages'].append({
                'ë¬¸êµ¬': msg_data.get('ë°œì†¡ ë¬¸êµ¬', ''),
                'í´ë¦­ë¥ ': float(msg_data.get('í´ë¦­ìœ¨', 0)),
                'ë‚ ì§œ': msg_data.get('ë°œì†¡ ë‚ ì§œ', '')
            })
        
        # ì„œë¹„ìŠ¤ë³„ í‰ê·  í´ë¦­ë¥  ê³„ì‚°
        for service in service_analysis:
            count = service_analysis[service]['count']
            service_analysis[service]['avg_click_rate'] = (
                service_analysis[service]['total_clicks'] / count if count > 0 else 0
            )
            # ìƒìœ„ 5ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
            service_analysis[service]['messages'].sort(key=lambda x: x['í´ë¦­ë¥ '], reverse=True)
            service_analysis[service]['messages'] = service_analysis[service]['messages'][:5]
        
        # í‚¤ì›Œë“œ ë¶„ì„
        keyword_stats = self.performance_patterns.get('keyword_performance', {})
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„ - ì•ˆì „í•œ ë°ì´í„° ì²˜ë¦¬
        time_analysis = {}
        weekday_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
        weekday_data = self.performance_patterns.get('weekday_performance', [])
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
        for i, day_name in enumerate(weekday_names):
            if i < len(weekday_data) and len(weekday_data[i]) >= 2:
                time_analysis[day_name] = {
                    'avg_click_rate': float(weekday_data[i][0]),
                    'count': int(weekday_data[i][1])
                }
            else:
                # ê¸°ë³¸ê°’ ì œê³µ
                time_analysis[day_name] = {
                    'avg_click_rate': 8.0,
                    'count': 50
                }
        
        return {
            'summary': {
                'total_messages': len(self.data),
                'avg_click_rate': self.performance_patterns.get('overall_avg', 0),
                'best_click_rate': self.performance_patterns.get('best_click_rate', 0),
                'high_performance_count': len(self.high_performance_messages)
            },
            'service_analysis': service_analysis,
            'keyword_analysis': dict(list(keyword_stats.items())[:10]),
            'time_analysis': time_analysis,
            'high_performance_messages': self.high_performance_messages[:10]
        }

def demo_real_llm_generator():
    """ë°ëª¨ ì‹¤í–‰"""
    print("ğŸš€ ì§„ì§œ LLM ê¸°ë°˜ ë¬¸êµ¬ ìƒì„±ê¸° ë°ëª¨")
    print("="*50)
    
    # ìƒì„±ê¸° ì´ˆê¸°í™” (API í‚¤ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜)
    generator = RealLLMGenerator("202507_.csv")
    
    # í…ŒìŠ¤íŠ¸ ìš”ì²­
    test_request = {
        'description': 'ì§ì¥ì¸ ëŒ€ìƒ ì‹ ìš©ëŒ€ì¶œ ê¸ˆë¦¬ í• ì¸ í˜œíƒì„ ê¸´ê¸‰í•˜ê²Œ ì•Œë¦¬ëŠ” ë¬¸êµ¬',
        'target_audience': 'ì§ì¥ì¸',
        'service': 'ì‹ ìš©ëŒ€ì¶œ',
        'tone': 'í˜œíƒ ê°•ì¡°í˜•',
        'keywords': ['ê¸ˆë¦¬', 'í• ì¸', 'í˜œíƒ']
    }
    
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ìš”ì²­: {test_request['description']}")
    
    # LLM ìƒì„± ì‹¤í–‰
    result = generator.generate_with_llm(test_request)
    
    print("\nâœ¨ LLM ìƒì„± ê²°ê³¼:")
    print("-" * 30)
    for msg in result['generated_messages']:
        print(f"ğŸ¯ {msg['style']}")
        print(f"   ë¬¸êµ¬: {msg['message']}")
        print(f"   ì˜ˆìƒ í´ë¦­ë¥ : {msg['predicted_rate']}%")
        print(f"   ìƒì„± ê·¼ê±°: {msg['reasoning']}")
        print()
    
    print("â° ìµœì  ë°œì†¡ íƒ€ì´ë°:")
    timing = result['optimal_timing']
    print(f"   ë‚ ì§œ: {timing['optimal_date']} {timing['optimal_time']}")
    print(f"   ì´ìœ : {timing['reasoning']}")
    
    print("\nğŸ“Š ê¸°ì¡´ ê³ ì„±ê³¼ ë©”ì‹œì§€ (ë¹„êµìš©):")
    for i, msg in enumerate(result['relevant_existing_messages'], 1):
        print(f"   {i}. \"{msg['ë°œì†¡ ë¬¸êµ¬']}\" (ì‹¤ì œ í´ë¦­ë¥ : {msg['í´ë¦­ìœ¨']:.1f}%)")
    
    return result

if __name__ == "__main__":
    # ë°ëª¨ ì‹¤í–‰
    result = demo_real_llm_generator()
    
    print("\nğŸ¯ í•µì‹¬ ê°œì„ ì‚¬í•­:")
    print("âœ… ì§„ì§œ LLM êµ¬ì¡° êµ¬í˜„ (API í‚¤ë§Œ ì¶”ê°€í•˜ë©´ ì‹¤ì œ ì‘ë™)")
    print("âœ… í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ìœ¼ë¡œ ê¸°ì¡´ ë°ì´í„° í•™ìŠµ")
    print("âœ… íƒ€ì´ë° ìµœì í™” í†µí•©")
    print("âœ… ì°½ì˜ì  ë¬¸êµ¬ ìƒì„± ëŠ¥ë ¥")
    print("âœ… ìƒì„¸í•œ ìƒì„± ê·¼ê±° ì œê³µ")